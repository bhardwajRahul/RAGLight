from __future__ import annotations
from typing import Optional, Dict, Any
from typing_extensions import override
import logging

from ..config.settings import Settings
from .llm import LLM
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

Settings.setup_logging()

class GeminiModel(LLM):
    """
    A subclass of LLM that uses Google's Gemini as the backend for text generation.

    This class provides an interface to interact with Google's Generative AI,
    enabling text generation with various models supported by the Gemini API.

    Attributes:
        model_name (str): The name of the model to use with Gemini.
        role (str): The role of the user in the conversation, typically "user".
        system_prompt (str): The system prompt to use for text generation.
        model (ChatGoogleGenerativeAI): The Gemini client configured to interact with the API.
    """

    def __init__(
        self,
        model_name: str,
        system_prompt: Optional[str] = None,
        system_prompt_file: Optional[str] = None,
        api_base: Optional[str] = None,
        role: str = "user",
    ) -> None:
        """
        Initializes an instance of GeminiModel.

        Args:
            model_name (str): The name of the model to use with Gemini.
            system_prompt (Optional[str]): The system prompt to use. If not provided, it will be loaded from system_prompt_file or use the default value.
            system_prompt_file (Optional[str]): The path to the file to load the system prompt from. If provided, it takes precedence over system_prompt.
            role (str): The role of the user in the conversation, defaults to "user".
        """
        self.api_base = api_base or Settings.DEFAULT_GOOGLE_CLIENT
        super().__init__(model_name, system_prompt, system_prompt_file, self.api_base)
        logging.info(f"Using Gemini with {model_name} model ğŸ¤–")
        self.role: str = role

    @override
    def load(self) -> ChatGoogleGenerativeAI:
        """
        Loads the Gemini client.

        Returns:
            ChatGoogleGenerativeAI: The client object to interact with Gemini.
        """
        client_options = (
            {"api_endpoint": self.api_base}
            if self.api_base
            else None
        )
        return ChatGoogleGenerativeAI(
            model=self.model_name,
            google_api_key=Settings.GEMINI_API_KEY,
            convert_system_message_to_human=True,
        )

    @override
    def generate(self, input: Dict[str, Any]) -> str:
        """
        Generates text using the Gemini model.

        Args:
            input (Dict[str, Any]): The input data for text generation.

        Returns:
            str: The text generated by the model.
        """
        

        messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=str(input)),
        ]
        response = self.model.invoke(messages)
        return response.content
